{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## symbols","metadata":{}},{"cell_type":"code","source":"graphemes = ['<pad>', '<s>', '</s>'] + list(\"abcdefghijklmnopqrstuvwxyz'-.\")\nphonemes = ['<pad>', '<s>', '</s>', 'AA', 'AA0', 'AA1', 'AA2', 'AE', 'AE0', 'AE1', 'AE2', 'AH', 'AH0', 'AH1', 'AH2',\n            'AO', 'AO0', 'AO1', 'AO2', 'AW', 'AW0', 'AW1', 'AW2', 'AY', 'AY0', 'AY1', 'AY2', 'B', 'CH', 'D', 'DH', 'EH',\n            'EH0', 'EH1', 'EH2', 'ER', 'ER0', 'ER1', 'ER2', 'EY', 'EY0', 'EY1', 'EY2', 'F', 'G', 'HH', 'IH', 'IH0',\n            'IH1', 'IH2', 'IY', 'IY0', 'IY1', 'IY2', 'JH', 'K', 'L', 'M', 'N', 'NG', 'OW', 'OW0', 'OW1', 'OW2', 'OY',\n            'OY0', 'OY1', 'OY2', 'P', 'R', 'S', 'SH', 'T', 'TH', 'UH', 'UH0', 'UH1', 'UH2', 'UW', 'UW0', 'UW1', 'UW2',\n            'V', 'W', 'Y', 'Z', 'ZH']\n\n# phonemes = ['<pad>', '<s>', '</s>']\n#\n# for line in open(\"data/cmudict.symbols\"):\n#     phonemes.append(line.strip('\\n'))\n#\n# print(phonemes)\n# print(graphemes)\n\ngraphemes_id2char = dict(enumerate(graphemes))\nphonemes_id2char = dict(enumerate(phonemes))\ngraphemes_char2id = dict((v, k) for k, v in enumerate(graphemes))\nphonemes_char2id = dict((v, k) for k, v in enumerate(phonemes))\n\n\ndef word2id(word):\n    return [graphemes_char2id[c] for c in list(word)]\n\n\ndef id2word(idx_list):\n    return ''.join([graphemes_id2char[idx] for idx in idx_list])\n\n\ndef phoneme2id(phoneme_seq):\n    return [phonemes_char2id[p] for p in phoneme_seq.split(' ')]\n\n\ndef id2phoneme(idx_list):\n    return ' '.join([phonemes_id2char[idx] for idx in idx_list])","metadata":{"execution":{"iopub.status.busy":"2022-05-01T08:02:57.849898Z","iopub.execute_input":"2022-05-01T08:02:57.850267Z","iopub.status.idle":"2022-05-01T08:02:57.871487Z","shell.execute_reply.started":"2022-05-01T08:02:57.850222Z","shell.execute_reply":"2022-05-01T08:02:57.87073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## config","metadata":{}},{"cell_type":"code","source":"# 超参配置\n# yaml\nclass Hyperparameter:\n    # ################################################################\n    #                             Data\n    # ################################################################\n    device = 'cuda'\n    data_root = './data/'\n    origin_dict_path = '../input/cmu-pronouncing-dictionary/cmudict.dict'\n    trainset_path = './data/data_train.json'\n    testset_path = './data/data_test.json'\n    devset_path = './data/data_val.json'\n\n    seed = 1234  # random seed\n\n    # ################################################################\n    #                             Model Structure\n    # ################################################################\n\n    encoder_layer_num = 6\n    encoder_dim = 128\n    encoder_drop_prob = 0.1\n    graphemes_size = len(graphemes_char2id)\n    encoder_max_input = 30\n\n    nhead = 4\n\n    encoder_feed_forward_dim = 1024\n    decoder_feed_forward_dim = 1024\n    feed_forward_drop_prob = 0.3\n\n    decoder_layer_num = 6\n    decoder_dim = 128\n    decoder_drop_prob = 0.1\n    phoneme_size = len(phonemes_char2id)\n    MAX_DECODE_STEP = 50\n\n    ENCODER_SOS_IDX = graphemes_char2id['<s>']\n    ENCODER_EOS_IDX = graphemes_char2id['</s>']\n    ENCODER_PAD_IDX = graphemes_char2id['<pad>']\n    DECODER_SOS_IDX = phonemes_char2id['<s>']\n    DECODER_EOS_IDX = phonemes_char2id['</s>']\n    DECODER_PAD_IDX = phonemes_char2id['<pad>']\n\n    # ################################################################\n    #                             Experiment\n    # ################################################################\n    batch_size = 128\n    init_lr = 1e-4\n    epochs = 100\n    verbose_step = 100\n    save_step = 500\n    grad_clip_thresh = 1.\n\n\nHP = Hyperparameter()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-01T08:02:57.875836Z","iopub.execute_input":"2022-05-01T08:02:57.876422Z","iopub.status.idle":"2022-05-01T08:02:57.892423Z","shell.execute_reply.started":"2022-05-01T08:02:57.876382Z","shell.execute_reply":"2022-05-01T08:02:57.891579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## preprocess","metadata":{}},{"cell_type":"code","source":"import json\nimport os\nimport random\n\nrandom.seed(HP.seed)\n\nfor foldername in ['data', 'log', 'model_save']:\n    if not os.path.exists(foldername):\n        os.mkdir(foldername)\n\ntrain_ratio, eval_ratio, test_ratio = 0.8, 0.1, 0.1\n\nlines = []\nfor line in open(HP.origin_dict_path):\n    lines.append(line.strip('\\n'))\nrandom.shuffle(lines)\n\nlength = len(lines)\n\ndef lines2dict(lines):\n    the_dict = {}\n    for line in lines:\n        contents = line.split(' ', 1)\n        if contents[0].endswith(')'):\n            continue\n        if '1' in contents[0]:\n            continue\n        if '#' in contents[1]:\n            continue\n        the_dict[contents[0]] = contents[1]\n    return the_dict\n\ntrainset_dict = lines2dict(lines[:int(length * train_ratio)])\nevalset_dict = lines2dict(lines[int(length * train_ratio):int(length * (train_ratio + eval_ratio))])\ntestset_dict = lines2dict(lines[int(length * (train_ratio + eval_ratio)):])\n\njson.dump(trainset_dict, open(HP.trainset_path, 'w'))\njson.dump(evalset_dict, open(HP.devset_path, 'w'))\njson.dump(testset_dict, open(HP.testset_path, 'w'))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-01T08:02:57.893887Z","iopub.execute_input":"2022-05-01T08:02:57.895759Z","iopub.status.idle":"2022-05-01T08:02:58.40142Z","shell.execute_reply.started":"2022-05-01T08:02:57.895728Z","shell.execute_reply":"2022-05-01T08:02:58.400669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## dataset_g2p","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport json\nimport torch\n\n\nclass G2PDataset(Dataset):\n\n    def __init__(self, dataset_path):\n        data_dict = json.load(open(dataset_path, 'r'))\n        self.data_pairs = list(data_dict.items())\n\n    def __getitem__(self, index):\n        word, phone_seq = self.data_pairs[index][0], self.data_pairs[index][1]\n        return word2id(word), phoneme2id(phone_seq)\n\n    def __len__(self):\n        return len(self.data_pairs)\n\n\ndef collate_fn(iter_batch):\n    N = len(iter_batch)\n    word_indexes, phoneme_indexs = [list(it) for it in zip(*iter_batch)]\n\n    [it.insert(0, graphemes_char2id['<s>']) for it in word_indexes]\n    [it.append(graphemes_char2id['</s>']) for it in word_indexes]\n\n    [it.insert(0, phonemes_char2id['<s>']) for it in phoneme_indexs]\n    [it.append(phonemes_char2id['</s>']) for it in phoneme_indexs]\n\n    word_lengths, sort_index = torch.sort(torch.tensor([len(it) for it in word_indexes]).long(), descending=True)\n    max_word_len = word_lengths[0]\n    word_padded = torch.zeros(size=(N, max_word_len)).long()\n\n    max_phoneme_len = max([len(it) for it in phoneme_indexs])\n    phoneme_padded = torch.zeros(size=(N, max_phoneme_len)).long()\n    phoneme_lengths = torch.zeros(size=(N,)).long()\n\n    for idx, idx_s in enumerate(sort_index.tolist()):\n        word_padded[idx][:word_lengths[idx]] = torch.tensor(word_indexes[idx_s]).long()\n        phoneme_padded[idx][:len(phoneme_indexs[idx_s])] = torch.tensor(phoneme_indexs[idx_s]).long()\n        phoneme_lengths[idx] = len(phoneme_indexs[idx_s])\n\n    return word_padded, word_lengths, phoneme_padded, phoneme_lengths\n","metadata":{"execution":{"iopub.status.busy":"2022-05-01T08:02:58.403204Z","iopub.execute_input":"2022-05-01T08:02:58.40346Z","iopub.status.idle":"2022-05-01T08:02:58.415457Z","shell.execute_reply.started":"2022-05-01T08:02:58.403426Z","shell.execute_reply":"2022-05-01T08:02:58.414788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## model","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport math\n\n\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, max_len=10000):\n        super(PositionalEncoding, self).__init__()\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.) / d_model))\n        position = torch.arange(max_len).unsqueeze(1)\n        pe = torch.zeros(1, max_len, d_model)\n        pe[0, :, 0::2] = torch.sin(position * div_term)\n        pe[0, :, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1), :]\n        return x\n\n\nclass Encoder(nn.Module):\n\n    def __init__(self):\n        super(Encoder, self).__init__()\n\n        self.token_embedding = nn.Embedding(HP.graphemes_size, HP.encoder_dim)\n        self.pe = PositionalEncoding(d_model=HP.encoder_dim, max_len=HP.encoder_max_input)\n        self.layers = nn.ModuleList([EncoderLayer() for _ in range(HP.encoder_layer_num)])\n        self.drop = nn.Dropout(HP.encoder_drop_prob)\n        self.register_buffer('scale', torch.sqrt(torch.tensor(HP.encoder_dim).float()))\n\n    def forward(self, inputs, inputs_mask):\n        token_embedded = self.token_embedding(inputs)\n        inputs = self.pe(token_embedded * self.scale)\n        inputs = self.drop(inputs)\n\n        for idx, layer in enumerate(self.layers):\n            inputs = layer(inputs, inputs_mask)\n\n        return inputs\n\n\nclass EncoderLayer(nn.Module):\n\n    def __init__(self):\n        super(EncoderLayer, self).__init__()\n\n        self.self_att_layer_norm = nn.LayerNorm(HP.encoder_dim)\n        self.pff_layer_norm = nn.LayerNorm(HP.encoder_dim)\n\n        self.self_att = MultiHeadAttentionLayer(HP.encoder_dim, HP.nhead)\n        self.pff = PointWiseFeedForwardLayer(HP.encoder_dim, HP.encoder_feed_forward_dim, HP.feed_forward_drop_prob)\n\n        self.dropout = nn.Dropout(HP.encoder_drop_prob)\n\n    def forward(self, inputs, inputs_mask):\n        _inputs, att_res = self.self_att(inputs, inputs, inputs, inputs_mask)\n        inputs = self.self_att_layer_norm(inputs + self.dropout(_inputs))\n        _inputs = self.pff(inputs)\n        inputs = self.pff_layer_norm(inputs + self.dropout(_inputs))\n        return inputs\n\n\nclass MultiHeadAttentionLayer(nn.Module):\n\n    def __init__(self, hidden_dim, nhead):\n        super(MultiHeadAttentionLayer, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.nhead = nhead\n        assert self.hidden_dim % self.nhead == 0\n        self.head_dim = self.hidden_dim // self.nhead\n\n        self.fc_q = nn.Linear(self.hidden_dim, self.hidden_dim)\n        self.fc_k = nn.Linear(self.hidden_dim, self.hidden_dim)\n        self.fc_v = nn.Linear(self.hidden_dim, self.hidden_dim)\n\n        self.fc_o = nn.Linear(self.hidden_dim, self.hidden_dim)\n        self.register_buffer('scale', torch.sqrt(torch.tensor(self.hidden_dim).float()))\n\n    def forward(self, query, key, value, input_mask=None):\n        bn = query.size(0)\n        Q = self.fc_q(query)\n        K = self.fc_k(key)\n        V = self.fc_v(value)\n\n        # split into n head\n        Q = Q.view(bn, -1, self.nhead, self.head_dim).permute(0, 2, 1, 3)\n        K = K.view(bn, -1, self.nhead, self.head_dim).permute(0, 2, 1, 3)\n        V = V.view(bn, -1, self.nhead, self.head_dim).permute(0, 2, 1, 3)\n\n        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n        if input_mask is not None:\n            energy = energy.masked_fill(input_mask == 0, -1.e10)\n        attention = F.softmax(energy, dim=-1)\n        out = torch.matmul(attention, V)\n        out = out.permute(0, 2, 1, 3).contiguous()\n        out = out.view(bn, -1, self.hidden_dim)\n        out = self.fc_o(out)\n        return out, attention\n\n\nclass PointWiseFeedForwardLayer(nn.Module):\n\n    def __init__(self, hidden_dim, pff_dim, pff_drop_prob):\n        super(PointWiseFeedForwardLayer, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.pff_dim = pff_dim\n        self.pff_drop_prob = pff_drop_prob\n\n        self.fc1 = nn.Linear(self.hidden_dim, self.pff_dim)\n        self.fc2 = nn.Linear(self.pff_dim, self.hidden_dim)\n        self.dropout = nn.Dropout(self.pff_drop_prob)\n\n    def forward(self, x):\n        x = self.dropout(F.relu(self.fc1(x)))\n        x = self.fc2(x)\n        return x\n\n\nclass Decoder(nn.Module):\n\n    def __init__(self):\n        super(Decoder, self).__init__()\n        self.token_embedding = nn.Embedding(HP.phoneme_size, HP.decoder_dim)\n        self.pe = PositionalEncoding(d_model=HP.decoder_dim, max_len=HP.MAX_DECODE_STEP)\n        self.layers = nn.ModuleList([DecoderLayer() for _ in range(HP.decoder_layer_num)])\n        self.fc_out = nn.Linear(HP.decoder_dim, HP.phoneme_size)\n        self.drop = nn.Dropout(HP.decoder_drop_prob)\n        self.register_buffer('scale', torch.sqrt(torch.tensor(HP.decoder_dim).float()))\n\n    def forward(self, target, enc_src, target_mask, src_mask):\n        global attention\n        token_embbed = self.token_embedding(target)\n        pos_embbed = self.pe(token_embbed)\n        target = self.drop(pos_embbed)\n\n        for idx, layer in enumerate(self.layers):\n            target, attention = layer(target, enc_src, target_mask, src_mask)\n        out = self.fc_out(target)\n        return out, attention\n\n\nclass DecoderLayer(nn.Module):\n\n    def __init__(self):\n        super(DecoderLayer, self).__init__()\n        self.mask_self_att = MultiHeadAttentionLayer(HP.decoder_dim, HP.nhead)\n        self.mask_self_norm = nn.LayerNorm(HP.decoder_dim)\n\n        self.mha = MultiHeadAttentionLayer(HP.decoder_dim, HP.nhead)\n        self.mha_norm = nn.LayerNorm(HP.decoder_dim)\n\n        self.pff = PointWiseFeedForwardLayer(HP.decoder_dim, HP.decoder_feed_forward_dim, HP.feed_forward_drop_prob)\n        self.pff_norm = nn.LayerNorm(HP.decoder_dim)\n\n        self.dropout = nn.Dropout(HP.decoder_drop_prob)\n\n    def forward(self, target, enc_src, target_mask, src_mask):\n        _target, _ = self.mask_self_att(target, target, target, target_mask)\n        target = self.mask_self_norm(target + self.dropout(_target))\n\n        _target, attention = self.mha(target, enc_src, enc_src, src_mask)\n        target = self.mha_norm(target + self.dropout(_target))\n\n        _target = self.pff(target)\n        target = self.pff_norm(target + self.dropout(_target))\n\n        return target, attention\n\n\nclass Transformer(nn.Module):\n\n    def __init__(self):\n        super(Transformer, self).__init__()\n\n        self.encoder = Encoder()\n        self.decoder = Decoder()\n\n    def forward(self, src, target):\n        src_mask = self.create_src_mask(src)\n        target_mask = self.create_target_mask(target)\n\n        enc_src = self.encoder(src, src_mask)\n        output, attention = self.decoder(target, enc_src, target_mask, src_mask)\n        return output, attention\n\n    def infer(self, src):\n        pass\n\n    @staticmethod\n    def create_src_mask(src):\n        mask = (src != HP.ENCODER_PAD_IDX).unsqueeze(1).unsqueeze(2).to(HP.device)\n        return mask\n\n    @staticmethod\n    def create_target_mask(target):\n        target_length = target.size(1)\n        pad_mask = (target != HP.DECODER_PAD_IDX).unsqueeze(1).unsqueeze(2).to(HP.device)\n        sub_mask = torch.tril(torch.ones(target_length, target_length, dtype=torch.uint8)).bool().to(HP.device)\n        target_mask = pad_mask & sub_mask\n        return target_mask\n","metadata":{"execution":{"iopub.status.busy":"2022-05-01T08:02:58.416762Z","iopub.execute_input":"2022-05-01T08:02:58.417151Z","iopub.status.idle":"2022-05-01T08:02:58.464102Z","shell.execute_reply.started":"2022-05-01T08:02:58.417116Z","shell.execute_reply":"2022-05-01T08:02:58.463131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## trainer","metadata":{}},{"cell_type":"code","source":"import os.path\nimport random\nimport torch\nimport numpy as np\nfrom tensorboardX import SummaryWriter\nfrom torch import nn\nfrom torch import optim\nfrom torch.utils.data import DataLoader\n\nlogger = SummaryWriter('./log')\n\n# seed init: 保证模型的可复现性\ntorch.manual_seed(HP.seed)\nrandom.seed(HP.seed)\nnp.random.seed(HP.seed)\ntorch.cuda.manual_seed(HP.seed)\n\n\ndef evaluate(model, devloader, crit):\n    model.eval()\n    sum_loss = 0.\n    with torch.no_grad():\n        for batch in devloader:\n            words_idxs, word_lens, phoneme_seqs_idxs, phoneme_len = batch\n            output_post, attention = model(words_idxs.to(HP.device), phoneme_seqs_idxs[:, :-1].to(HP.device))\n            out = output_post.view(-1, output_post.size(-1))\n            target = phoneme_seqs_idxs[:, 1:]\n            target = target.contiguous().view(-1)\n            loss = crit(out.to(HP.device), target.to(HP.device))\n            sum_loss += loss.item()\n\n    model.train()\n    return sum_loss / len(devloader)\n\n\ndef save_checkpoint(model, epoch, opt, save_path):\n    save_dict = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': opt.state_dict()\n    }\n    torch.save(save_dict, save_path)\n\n\ndef train():\n\n    model = Transformer().to(HP.device)\n\n    criterion = nn.CrossEntropyLoss(ignore_index=HP.DECODER_PAD_IDX)\n\n    opt = optim.Adam(model.parameters(), lr=HP.init_lr)\n\n    trainset = G2PDataset(HP.trainset_path)\n    train_loader = DataLoader(trainset, batch_size=HP.batch_size, shuffle=True, drop_last=True, collate_fn=collate_fn)\n\n    devset = G2PDataset(HP.testset_path)\n    dev_loader = DataLoader(devset, batch_size=HP.batch_size, shuffle=True, drop_last=False, collate_fn=collate_fn)\n\n    start_epoch, step = 0, 0\n\n    model.train()\n\n    for epoch in range(start_epoch, HP.epochs):\n        print('Start Epoch: %d, Steps: %d' % (epoch, len(train_loader)))\n        for batch in train_loader:\n            words_idxs, word_lens, phoneme_seqs_idxs, phoneme_len = batch\n            opt.zero_grad()\n            output_post, attention = model(words_idxs.to(HP.device), phoneme_seqs_idxs[:, :-1].to(HP.device))\n            out = output_post.view(-1,output_post.size(-1))\n            target = phoneme_seqs_idxs[:,1:]\n            target = target.contiguous().view(-1)\n            loss = criterion(out.to(HP.device), target.to(HP.device))\n\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), HP.grad_clip_thresh)\n            opt.step()\n\n            logger.add_scalar('Loss/Train', loss, step)\n\n            if not step % HP.verbose_step:\n                eval_loss = evaluate(model, dev_loader, criterion)\n                logger.add_scalar('Loss/Dev', eval_loss, step)\n\n            if not step % HP.save_step:\n                model_path = 'model_%d_%d.model' % (epoch, step)\n                save_checkpoint(model, epoch, opt, os.path.join('model_save', model_path))\n\n            step += 1\n            logger.flush()\n            print('Epoch:[%d/%d], step:%d, Train Loss:%.5f, Dev Loss:%.5f' % (\n                epoch, HP.epochs, step, loss.item(), eval_loss))\n\n    logger.close()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-01T08:02:58.468347Z","iopub.execute_input":"2022-05-01T08:02:58.468557Z","iopub.status.idle":"2022-05-01T08:02:58.490085Z","shell.execute_reply.started":"2022-05-01T08:02:58.468533Z","shell.execute_reply":"2022-05-01T08:02:58.489392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 训练","metadata":{}},{"cell_type":"code","source":"train()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T08:02:58.493452Z","iopub.execute_input":"2022-05-01T08:02:58.493849Z","iopub.status.idle":"2022-05-01T08:04:12.260868Z","shell.execute_reply.started":"2022-05-01T08:02:58.493824Z","shell.execute_reply":"2022-05-01T08:04:12.259717Z"},"trusted":true},"execution_count":null,"outputs":[]}]}